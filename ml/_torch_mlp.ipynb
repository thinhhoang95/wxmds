{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FullyConnectedNetwork(\n",
      "  (fc1): Linear(in_features=625, out_features=512, bias=True)\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (fc3): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (fc4): Linear(in_features=128, out_features=15, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FullyConnectedNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        \n",
    "        # First fully connected layer\n",
    "        self.fc1 = nn.Linear(25 * 25, 512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "\n",
    "        # Second fully connected layer\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Third fully connected layer\n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "\n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(128, 15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten the image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Pass through the first layer, apply ReLU and dropout\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Second layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Third layer\n",
    "        x = F.relu(self.fc3(x))\n",
    "\n",
    "        # Output layer with softmax activation\n",
    "        x = self.fc4(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x # x is log probability\n",
    "\n",
    "# Create the neural network\n",
    "model = FullyConnectedNetwork()\n",
    "\n",
    "# Print the model structure\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a custom DataLoader for our dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StormShadow(torch.utils.data.Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx, :, :]\n",
    "        label = self.labels[idx, :]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Early stopping logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = np.Inf\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                \n",
    "early_stopping = EarlyStopping(patience=50, min_delta=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5625/5625 [00:00<00:00, 520402.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import xarray as xr \n",
    "vil_ds = xr.open_zarr('../hrrr17x.zarr/')\n",
    "vil_dr = vil_ds.VIL\n",
    "# Get all the file names in bystormlabels folder\n",
    "from tqdm import tqdm\n",
    "import numpy as np \n",
    "import os\n",
    "\n",
    "labels = np.zeros((5625, 15))\n",
    "\n",
    "for date_str in tqdm(vil_dr.attrs['date']):\n",
    "    # Replace : with _ in date_str\n",
    "    filename_to_open = date_str.replace(':', '_')\n",
    "    # check if the file exists\n",
    "    if not os.path.isfile('bystormlabels/' + filename_to_open + '.npz'):\n",
    "        continue\n",
    "    labell = np.load('bystormlabels/' + filename_to_open + '.npz')['arr_0']\n",
    "    frequency_vector = np.array([(labell==i).sum() for i in range(15)])\n",
    "    if frequency_vector.sum() == 0:\n",
    "        continue\n",
    "    else:\n",
    "        print(vil_dr.attrs['date'].index(date_str))\n",
    "    labels[vil_dr.attrs['date'].index(date_str)] = frequency_vector / frequency_vector.sum()\n",
    "vil_input = vil_dr.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensor\n",
    "vil_tensor = torch.from_numpy(vil_input)\n",
    "labels_tensor = torch.from_numpy(labels) # in normal form, NOT log probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vil_input_by_max(vil_tensor):\n",
    "    vil_tensor = vil_tensor - vil_tensor.min()\n",
    "    vil_tensor = vil_tensor / vil_tensor.max()\n",
    "    return vil_tensor\n",
    "\n",
    "def normalize_vil_input_by_mean_and_std(vil_tensor):\n",
    "    vil_tensor = vil_tensor - vil_tensor.mean()\n",
    "    vil_tensor = vil_tensor / vil_tensor.std()\n",
    "    return vil_tensor\n",
    "\n",
    "vil_tensor = normalize_vil_input_by_mean_and_std(vil_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data length:  4500\n",
      "Val data length:  562\n",
      "Test data length:  563\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training, validation and test sets\n",
    "train_split = 0.8\n",
    "val_split = 0.1\n",
    "test_split = 0.1\n",
    "\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "# Calculate lengths of splits\n",
    "train_length = int(train_split * len(vil_tensor))\n",
    "val_length = int(val_split * len(vil_tensor))\n",
    "test_length = len(vil_tensor) - train_length - val_length\n",
    "\n",
    "# Split the data\n",
    "train_data, val_data, test_data = random_split(vil_tensor, [train_length, val_length, test_length])\n",
    "\n",
    "# Print the size of the splits\n",
    "print('Train data length: ', len(train_data))\n",
    "print('Val data length: ', len(val_data))\n",
    "print('Test data length: ', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "# Create dataset\n",
    "dataset = StormShadow(vil_tensor, labels_tensor)\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "model = FullyConnectedNetwork()\n",
    "\n",
    "# Define the loss\n",
    "criterion = nn.KLDivLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of epochs to train the model\n",
    "n_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard setup\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/stormshadow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/thinhhoang/miniforge3/envs/traffic/lib/python3.9/site-packages/torch/nn/functional.py:2943: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 2/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 3/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 4/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 5/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 6/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 7/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 8/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 9/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Epoch 10/10, Training Loss: 0.0000, Validation Loss: 0.0000\n",
      "Test Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train and Evaluation Loops\n",
    "\n",
    "def train_one_epoch(model, train_loader, loss_function, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(data)\n",
    "        loss = loss_function(output, target)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate(model, data_loader, loss_function, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = loss_function(output, target)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def train_model(model, train_loader, valid_loader, loss_function, optimizer, num_epochs, device):\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_one_epoch(model, train_loader, loss_function, optimizer, device)\n",
    "        valid_loss = evaluate(model, valid_loader, loss_function, device)\n",
    "        \n",
    "        # Record the training loss\n",
    "        writer.add_scalar('Loss/train', train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Loss/valid', valid_loss / len(valid_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/train', 1 - train_loss / len(train_loader), epoch)\n",
    "        writer.add_scalar('Accuracy/valid', 1 - valid_loss / len(valid_loader), epoch)\n",
    "        \n",
    "        # Early stopping\n",
    "        early_stopping(valid_loss)\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {valid_loss:.4f}')\n",
    "\n",
    "# Assuming a CUDA-capable device is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10  # Example number of epochs\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f'Test Loss: {test_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
